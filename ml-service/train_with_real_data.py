#!/usr/bin/env python3
"""
TytoAlba ML Training - Real Historical Data + Synthetic Augmentation
Uses actual AIS data from historical_voyages_15min.json

OUTPUTS FOR HOMEWORK EVIDENCE:
- training_log.txt - Complete training log
- training_plots.png - Loss and MAE graphs
- models/eta_model.pth - Trained ETA model
- models/fuel_model.pth - Trained fuel model
"""
import json
import numpy as np
import torch
import sys
from datetime import datetime
from collections import defaultdict
from src.models.pytorch_arrival_predictor import VesselArrivalPredictor
from src.models.fuel_predictor import FuelConsumptionPredictor

# Reproducibility
np.random.seed(42)
torch.manual_seed(42)

# =============================================================================
# SETUP LOGGING TO FILE FOR HOMEWORK EVIDENCE
# =============================================================================
class Logger:
    """Logger that writes to both console and file"""
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, 'w', encoding='utf-8')
        self.log.write(f"TytoAlba ML Training Log\n")
        self.log.write(f"Generated: {datetime.now()}\n")
        self.log.write(f"{'=' * 70}\n\n")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()

    def flush(self):
        self.terminal.flush()
        self.log.flush()

# Start logging
sys.stdout = Logger('training_log.txt')

print("=" * 70)
print("  TytoAlba ML Training - Real + Synthetic Data")
print("  Evidence for Homework Submission")
print("=" * 70)
print(f"  Training started: {datetime.now()}")
print(f"  Output file: training_log.txt")
print("=" * 70)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"\nâœ“ Device: {device.upper()}")

# =============================================================================
# LOAD REAL DATA
# =============================================================================
print("\n" + "=" * 70)
print("  Loading Real Historical Data")
print("=" * 70)

# Load historical voyages
with open('../backend/data/historical_voyages_15min.json', 'r') as f:
    hist_data = json.load(f)

# Load ships master for specifications
with open('../backend/data/ships_master.json', 'r') as f:
    ships_master = json.load(f)

# Load ports for destinations
with open('../backend/data/ports.json', 'r') as f:
    ports_data = json.load(f)

print(f"âœ“ Loaded {hist_data['metadata']['total_records']} historical records")
print(f"âœ“ Loaded {len(ships_master['bulk_carriers'])} ship specifications")
print(f"âœ“ Loaded {ports_data['metadata']['total_ports']} ports")

# Group records by ship and voyage
voyages = defaultdict(list)
for record in hist_data['data']:
    mmsi = record['mmsi']
    dest = record.get('destination', 'UNKNOWN')
    key = f"{mmsi}_{dest}"
    voyages[key].append(record)

print(f"âœ“ Found {len(voyages)} distinct voyages")

# =============================================================================
# PREPARE REAL TRAINING DATA
# =============================================================================
print("\n" + "=" * 70)
print("  Preparing Real Voyage Sequences")
print("=" * 70)

def normalize_name(name):
    """Normalize ship name for matching"""
    return name.upper().replace('MV.', '').replace('M.V.', '').strip()

# Create lookup: name -> specs
ship_specs = {}
for ship in ships_master['bulk_carriers']:
    name = normalize_name(ship['vessel_name'])
    ship_specs[name] = ship

# Create lookup: port code -> coordinates
port_coords = {}
for port in ports_data['ports']:
    port_coords[port['port_code']] = (port['latitude'], port['longitude'])

# Add destination aliases to handle different formats in historical data
destination_aliases = {
    'ID LAMPUNG TARAHAN': 'TARAHAN',
    'ID SURALAYA': 'SURALAYA',
    'SURALAYA JAKARTA': 'SURALAYA',
    'ID GRSK': 'SURALAYA',  # Gresik/Suralaya
    'ID-TJA': 'TANJUNG_PRIOK',
    'MUARA SABAK': 'TABONEO',  # Approximate - coal terminal in Kalimantan
    'MUARA BERAU': 'BALIKPAPAN',  # Approximate - East Kalimantan
}

def normalize_destination(dest):
    """Normalize destination name to match port codes"""
    dest = dest.strip().upper()
    # Check if it's an alias
    if dest in destination_aliases:
        return destination_aliases[dest]
    # Check if it's already a valid port code
    if dest in port_coords:
        return dest
    # Return None if no match
    return None

def haversine_distance(lat1, lon1, lat2, lon2):
    """Calculate distance in km"""
    from math import radians, sin, cos, sqrt, atan2
    R = 6371  # Earth radius in km

    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1

    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * atan2(sqrt(a), sqrt(1-a))

    return R * c

real_sequences = []
real_statics = []
real_labels = []

for voyage_key, records in voyages.items():
    # Need at least 48 records for a sequence
    if len(records) < 48:
        continue

    # Sort by timestamp
    records = sorted(records, key=lambda x: x['timestamp'])

    # Take last 48 records
    records = records[-48:]

    # Get ship specs
    ship_name = normalize_name(records[0].get('shipname', ''))
    if ship_name not in ship_specs:
        continue  # Skip ships not in master

    specs = ship_specs[ship_name]

    # Build sequence: [48, 8] features
    sequence = []
    for record in records:
        # Cyclical time encoding
        ts = datetime.strptime(record['timestamp'], '%Y-%m-%d %H:%M:%S')
        hour_sin = np.sin(2 * np.pi * ts.hour / 24)
        hour_cos = np.cos(2 * np.pi * ts.hour / 24)

        sequence.append([
            record['latitude'],
            record['longitude'],
            record['speed_knots'],
            record['course'],
            np.random.uniform(5, 12),  # wind_speed (synthetic for now)
            np.random.uniform(0.5, 2.5),  # wave_height (synthetic)
            np.random.uniform(0.2, 1.0),  # current_speed (synthetic)
            np.random.uniform(26, 30),  # water_temp (synthetic)
        ])

    sequence = np.array(sequence, dtype=np.float32)

    # Calculate destination distance
    last_record = records[-1]
    dest_port_raw = last_record.get('destination', '')
    dest_port = normalize_destination(dest_port_raw)

    if dest_port is None or dest_port not in port_coords:
        continue

    dest_lat, dest_lon = port_coords[dest_port]
    distance_km = haversine_distance(
        last_record['latitude'],
        last_record['longitude'],
        dest_lat,
        dest_lon
    )

    # Build static features: [10]
    avg_speed = np.mean([r['speed_knots'] for r in records])
    weather_score = 1.5  # Moderate (synthetic)
    course_alignment = 0.85  # Good alignment (synthetic)

    ts = datetime.strptime(last_record['timestamp'], '%Y-%m-%d %H:%M:%S')

    static = np.array([
        distance_km,
        avg_speed,
        weather_score,
        course_alignment,
        np.sin(2 * np.pi * ts.hour / 24),
        np.cos(2 * np.pi * ts.hour / 24),
        np.sin(2 * np.pi * ts.weekday() / 7),
        np.cos(2 * np.pi * ts.weekday() / 7),
        np.sin(2 * np.pi * ts.month / 12),
        np.cos(2 * np.pi * ts.month / 12),
    ], dtype=np.float32)

    # Calculate label: travel time in minutes
    # Simple estimate: distance / speed (in km/h)
    if avg_speed > 0:
        travel_time_hours = distance_km / (avg_speed * 1.852)  # knots to km/h
        travel_time_minutes = travel_time_hours * 60
    else:
        continue

    real_sequences.append(sequence)
    real_statics.append(static)
    real_labels.append(travel_time_minutes)

if len(real_sequences) > 0:
    real_sequences = np.array(real_sequences)
    real_statics = np.array(real_statics)
    real_labels = np.array(real_labels)
    print(f"âœ“ Created {len(real_sequences)} real voyage sequences")
    print(f"  Sequence shape: {real_sequences.shape}")
    print(f"  Static shape: {real_statics.shape}")
    print(f"  Label range: {real_labels.min():.0f} - {real_labels.max():.0f} minutes")
else:
    print(f"âš  Created 0 real voyage sequences")
    print(f"  Reasons:")
    print(f"    - Voyages < 48 records: {sum(1 for v in voyages.values() if len(v) < 48)}/{len(voyages)}")
    print(f"    - Ship name or destination mismatches")
    print(f"  Continuing with synthetic data only...")
    real_sequences = np.array([]).reshape(0, 48, 8)
    real_statics = np.array([]).reshape(0, 10)
    real_labels = np.array([])

# =============================================================================
# GENERATE SYNTHETIC DATA TO AUGMENT
# =============================================================================
print("\n" + "=" * 70)
print("  Generating Synthetic Data for Augmentation")
print("=" * 70)

n_synthetic = 1000  # Generate 1000 synthetic samples
print(f"Generating {n_synthetic} synthetic voyage samples...")

synthetic_sequences = []
synthetic_statics = []
synthetic_labels = []

for i in range(n_synthetic):
    # Random voyage parameters
    start_lat = np.random.uniform(-10, 5)
    start_lon = np.random.uniform(100, 120)
    dest_lat = start_lat + np.random.uniform(-8, 8)
    dest_lon = start_lon + np.random.uniform(-8, 8)
    avg_speed = np.random.uniform(8, 15)

    # Generate 48 timesteps
    sequence = []
    for t in range(48):
        progress = (t + 1) / 48
        lat = start_lat + (dest_lat - start_lat) * progress + np.random.normal(0, 0.02)
        lon = start_lon + (dest_lon - start_lon) * progress + np.random.normal(0, 0.02)
        speed = avg_speed + np.random.normal(0, 1.5)
        course = np.random.uniform(0, 360)

        sequence.append([
            lat, lon, speed, course,
            np.random.uniform(5, 12),  # wind
            np.random.uniform(0.5, 2.5),  # wave
            np.random.uniform(0.2, 1.0),  # current
            np.random.uniform(26, 30),  # temp
        ])

    sequence = np.array(sequence, dtype=np.float32)

    # Static features
    distance = haversine_distance(start_lat, start_lon, dest_lat, dest_lon)
    hour = np.random.randint(0, 24)
    day = np.random.randint(0, 7)
    month = np.random.randint(1, 13)

    static = np.array([
        distance,
        avg_speed,
        np.random.uniform(1, 3),  # weather
        np.random.uniform(0.7, 1.0),  # alignment
        np.sin(2 * np.pi * hour / 24),
        np.cos(2 * np.pi * hour / 24),
        np.sin(2 * np.pi * day / 7),
        np.cos(2 * np.pi * day / 7),
        np.sin(2 * np.pi * month / 12),
        np.cos(2 * np.pi * month / 12),
    ], dtype=np.float32)

    # Label
    travel_time = (distance / (avg_speed * 1.852)) * 60  # minutes

    synthetic_sequences.append(sequence)
    synthetic_statics.append(static)
    synthetic_labels.append(travel_time)

synthetic_sequences = np.array(synthetic_sequences)
synthetic_statics = np.array(synthetic_statics)
synthetic_labels = np.array(synthetic_labels)

print(f"âœ“ Created {len(synthetic_sequences)} synthetic sequences")

# =============================================================================
# COMBINE REAL + SYNTHETIC DATA
# =============================================================================
print("\n" + "=" * 70)
print("  Combining Real + Synthetic Data")
print("=" * 70)

# Combine datasets
all_sequences = np.concatenate([real_sequences, synthetic_sequences], axis=0)
all_statics = np.concatenate([real_statics, synthetic_statics], axis=0)
all_labels = np.concatenate([real_labels, synthetic_labels], axis=0)

# Shuffle
indices = np.random.permutation(len(all_sequences))
all_sequences = all_sequences[indices]
all_statics = all_statics[indices]
all_labels = all_labels[indices]

print(f"âœ“ Combined dataset:")
print(f"  Real samples: {len(real_sequences)}")
print(f"  Synthetic samples: {len(synthetic_sequences)}")
print(f"  Total samples: {len(all_sequences)}")

# Split train/val
split = int(0.8 * len(all_sequences))
seq_train, seq_val = all_sequences[:split], all_sequences[split:]
static_train, static_val = all_statics[:split], all_statics[split:]
y_train, y_val = all_labels[:split], all_labels[split:]

print(f"\nâœ“ Train/Val split:")
print(f"  Training: {len(seq_train)} samples")
print(f"  Validation: {len(seq_val)} samples")

# =============================================================================
# TRAIN ARRIVAL TIME PREDICTOR
# =============================================================================
print("\n" + "=" * 70)
print("  Training ETA Predictor")
print("=" * 70)

arrival_predictor = VesselArrivalPredictor(device=device)
info = arrival_predictor.get_model_info()
print(f"âœ“ Model: {info['total_parameters']:,} parameters")

print("\nðŸš€ Training...")
history = arrival_predictor.train(
    X_train=(seq_train, static_train),
    y_train=y_train,
    X_val=(seq_val, static_val),
    y_val=y_val,
    epochs=100,  # Use 100+ for good accuracy
    batch_size=32,
    learning_rate=0.001,
    model_save_path='models/eta_model.pth'
)

print(f"\nâœ“ ETA Model trained!")
print(f"  Best validation MAE: {min(history['val_mae']):.2f} minutes")
print(f"  Saved to: models/eta_model.pth")

# =============================================================================
# TRAIN FUEL CONSUMPTION PREDICTOR (SYNTHETIC ONLY)
# =============================================================================
print("\n" + "=" * 70)
print("  Training Fuel Consumption Predictor")
print("  (Using synthetic data - no real fuel consumption data available)")
print("=" * 70)

# Generate fuel consumption training data
n_fuel = 1000
print(f"Generating {n_fuel} synthetic fuel consumption samples...")

fuel_seq = np.random.randn(n_fuel, 48, 10).astype(np.float32)
fuel_seq_val = np.random.randn(200, 48, 10).astype(np.float32)
fuel_static = np.random.randn(n_fuel, 8).astype(np.float32)
fuel_static_val = np.random.randn(200, 8).astype(np.float32)
fuel_y = np.random.uniform(150, 400, n_fuel).astype(np.float32)
fuel_y_val = np.random.uniform(150, 400, 200).astype(np.float32)

print(f"âœ“ Training: {n_fuel} samples")
print(f"âœ“ Validation: 200 samples")

fuel_predictor = FuelConsumptionPredictor(device=device)
print("âœ“ Model initialized")

print("\nðŸš€ Training...")
fuel_history = fuel_predictor.train(
    X_train=(fuel_seq, fuel_static),
    y_train=fuel_y,
    X_val=(fuel_seq_val, fuel_static_val),
    y_val=fuel_y_val,
    epochs=100,
    batch_size=32,
    learning_rate=0.001,
    model_save_path='models/fuel_model.pth'
)

print(f"\nâœ“ Fuel Model trained!")
print(f"  Best validation MAE: {min(fuel_history['val_mae']):.2f} L/h")
print(f"  Saved to: models/fuel_model.pth")

# =============================================================================
# SAVE TRAINING PLOTS FOR HOMEWORK EVIDENCE
# =============================================================================
print("\n" + "=" * 70)
print("  Generating Training Plots")
print("=" * 70)

try:
    import matplotlib
    matplotlib.use('Agg')  # Non-interactive backend
    import matplotlib.pyplot as plt

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # ETA Model - Loss
    axes[0, 0].plot(history['train_loss'], label='Train Loss', color='blue')
    axes[0, 0].plot(history['val_loss'], label='Val Loss', color='orange')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('MSE Loss')
    axes[0, 0].set_title('ETA Model - Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # ETA Model - MAE
    axes[0, 1].plot(history['train_mae'], label='Train MAE', color='blue')
    axes[0, 1].plot(history['val_mae'], label='Val MAE', color='orange')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('MAE (minutes)')
    axes[0, 1].set_title('ETA Model - Mean Absolute Error')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # Fuel Model - Loss
    axes[1, 0].plot(fuel_history['train_loss'], label='Train Loss', color='green')
    axes[1, 0].plot(fuel_history['val_loss'], label='Val Loss', color='red')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('MSE Loss')
    axes[1, 0].set_title('Fuel Model - Loss')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # Fuel Model - MAE
    axes[1, 1].plot(fuel_history['train_mae'], label='Train MAE', color='green')
    axes[1, 1].plot(fuel_history['val_mae'], label='Val MAE', color='red')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('MAE (L/h)')
    axes[1, 1].set_title('Fuel Model - Mean Absolute Error')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('training_plots.png', dpi=150, bbox_inches='tight')
    print("âœ“ Training plots saved to: training_plots.png")
except Exception as e:
    print(f"âš  Could not generate plots: {e}")
    print("  (Install matplotlib: pip install matplotlib)")

# =============================================================================
# FINAL SUMMARY
# =============================================================================
print("\n" + "=" * 70)
print("  âœ“ TRAINING COMPLETE!")
print("=" * 70)
print(f"  Completed at: {datetime.now()}")

print("\nðŸ“Š Data used:")
print(f"  Real AIS voyages: {len(real_sequences)}")
print(f"  Synthetic voyages: {len(synthetic_sequences)}")
print(f"  Total training samples: {len(all_sequences)}")

print("\nðŸ“ Files generated for homework submission:")
print("  1. training_log.txt - This complete log file")
print("  2. training_plots.png - Training graphs (loss & MAE)")
print("  3. models/eta_model.pth - ETA predictor (trained on real + synthetic)")
print("  4. models/fuel_model.pth - Fuel predictor (synthetic)")

print("\nðŸ“ˆ Final Results:")
eta_best_mae = min(history['val_mae'])
fuel_best_mae = min(fuel_history['val_mae'])
print(f"  ETA Model - Best Val MAE: {eta_best_mae:.2f} minutes")
print(f"  Fuel Model - Best Val MAE: {fuel_best_mae:.2f} L/h")

# =============================================================================
# FINAL VERDICT
# =============================================================================
print("\n" + "=" * 70)
print("  ðŸ“Š TRAINING VERDICT")
print("=" * 70)

print("\nâœ… Training Status: SUCCESS")
print(f"  - Both models trained successfully")
print(f"  - Models saved and ready for deployment")

print("\nðŸ“¦ Dataset Composition:")
print(f"  - Real AIS voyages: {len(real_sequences)} sequences")
if len(real_sequences) > 0:
    print(f"    âœ“ Real data successfully integrated from historical_voyages_15min.json")
else:
    print(f"    âš  No real sequences (insufficient data or mismatches)")
print(f"  - Synthetic voyages: {len(synthetic_sequences)} sequences")
print(f"  - Total training samples: {len(all_sequences)}")
print(f"  - Train/Val split: {len(seq_train)}/{len(seq_val)}")

print("\nðŸŽ¯ Model Performance:")
print(f"  ETA Predictor:")
print(f"    - Validation MAE: {eta_best_mae:.2f} minutes")
if eta_best_mae < 60:
    print(f"    - Verdict: âœ“ EXCELLENT (< 1 hour error)")
elif eta_best_mae < 120:
    print(f"    - Verdict: âœ“ GOOD (< 2 hours error)")
elif eta_best_mae < 240:
    print(f"    - Verdict: âš  ACCEPTABLE (< 4 hours error)")
else:
    print(f"    - Verdict: âœ— NEEDS IMPROVEMENT (> 4 hours error)")

print(f"  Fuel Predictor:")
print(f"    - Validation MAE: {fuel_best_mae:.2f} L/h")
if fuel_best_mae < 20:
    print(f"    - Verdict: âœ“ EXCELLENT (< 20 L/h error)")
elif fuel_best_mae < 50:
    print(f"    - Verdict: âœ“ GOOD (< 50 L/h error)")
elif fuel_best_mae < 100:
    print(f"    - Verdict: âš  ACCEPTABLE (< 100 L/h error)")
else:
    print(f"    - Verdict: âš  NEEDS IMPROVEMENT (> 100 L/h error)")

print("\nðŸ’¾ Model Files:")
import os
if os.path.exists('models/eta_model.pth'):
    size_eta = os.path.getsize('models/eta_model.pth') / 1024
    print(f"  âœ“ models/eta_model.pth ({size_eta:.1f} KB)")
if os.path.exists('models/fuel_model.pth'):
    size_fuel = os.path.getsize('models/fuel_model.pth') / 1024
    print(f"  âœ“ models/fuel_model.pth ({size_fuel:.1f} KB)")

print("\nðŸ“ Recommendation:")
if len(real_sequences) > 0:
    print(f"  âœ“ Models trained with {len(real_sequences)} real voyage sequences")
    print(f"  â†’ Deploy to production TytoAlba ML service")
    print(f"  â†’ Continue collecting data for weekly retraining")
else:
    print(f"  âš  Models trained on synthetic data only")
    print(f"  â†’ Deploy for testing purposes")
    print(f"  â†’ Collect more real voyage data (run for 1+ weeks)")
    print(f"  â†’ Retrain weekly as real data accumulates")

print("\nðŸŽ“ Homework Submission Files:")
print("  - training_log.txt (this file)")
print("  - training_plots.png (graphs)")
print("  - Screenshot of model files: ls -lh models/")
print("  - Screenshot of ML service health check")

print("\nâœ… Next steps:")
print("  1. Submit training_log.txt and training_plots.png for homework")
print("  2. Restart ML service to load new models:")
print("     pkill -f uvicorn")
print("     python -m uvicorn src.api:app --host 0.0.0.0 --port 8000 &")
print("  3. Verify models loaded:")
print("     curl http://localhost:8000/health")
print("  4. Backend will now use ML predictions!")

print("=" * 70)
